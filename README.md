# ML/DL Training Pipeline Framework

A generic, reusable training pipeline framework for machine learning and deep learning models with built-in visualization and evaluation capabilities.

## Features

- **Generic Training Pipeline**: Modular framework supporting both traditional ML and deep learning models
- **CPU Allocation Model**: Specialized implementation for CPU allocation tasks
- **Comprehensive Visualization**: Training metrics, model performance, and inference visualization
- **Evaluation Suite**: Model assessment with multiple metrics and validation techniques
- **Configuration Management**: YAML-based configuration with Hydra integration
- **Hyperparameter Optimization**: Built-in support for Optuna
- **Experiment Tracking**: Integration with TensorBoard and Weights & Biases
- **ğŸ”® Advanced Inference Engine**: Production-ready inference with monitoring, validation, and drift detection
- **ğŸ” Model Validation Framework**: Comprehensive validation pipelines with robustness testing
- **ğŸš€ Deployment Manager**: Integrated deployment solution with health monitoring and serving capabilities
- **ğŸ“Š Performance Analytics**: Real-time performance monitoring and automated reporting
- **ğŸ›¡ï¸ Data Quality Validation**: Automated input validation and data quality assessment
- **ğŸ“ˆ Model Interpretability**: Feature importance analysis and explainability support

## Project Structure

```
gymnasium/
â”œâ”€â”€ src/                        # Core framework source code
â”‚   â”œâ”€â”€ core/                   # Core framework components
â”‚   â”‚   â”œâ”€â”€ pipeline.py         # Main training pipeline
â”‚   â”‚   â”œâ”€â”€ base_model.py       # Base model interface
â”‚   â”‚   â”œâ”€â”€ trainer.py          # Training logic
â”‚   â”‚   â”œâ”€â”€ evaluator.py        # Evaluation logic
â”‚   â”‚   â”œâ”€â”€ inference.py        # ğŸ”® Advanced inference engine
â”‚   â”‚   â”œâ”€â”€ validation.py       # ğŸ” Model validation framework
â”‚   â”‚   â””â”€â”€ deployment.py       # ğŸš€ Deployment manager
â”‚   â”œâ”€â”€ models/                 # Model implementations
â”‚   â”‚   â”œâ”€â”€ cpu_allocation.py   # CPU allocation specific models
â”‚   â”‚   â””â”€â”€ registry.py         # Model registry
â”‚   â”œâ”€â”€ data/                   # Data handling
â”‚   â”‚   â”œâ”€â”€ data_loader.py      # Data loading utilities
â”‚   â”‚   â””â”€â”€ preprocessor.py     # Data preprocessing
â”‚   â”œâ”€â”€ visualization/          # Visualization components
â”‚   â”‚   â”œâ”€â”€ training_viz.py     # Training visualization
â”‚   â”‚   â”œâ”€â”€ evaluation_viz.py   # Evaluation visualization
â”‚   â”‚   â””â”€â”€ inference_viz.py    # Inference visualization
â”‚   â””â”€â”€ utils/                  # Utility functions
â”‚       â”œâ”€â”€ config.py           # Configuration management
â”‚       â”œâ”€â”€ metrics.py          # Custom metrics
â”‚       â””â”€â”€ logging.py          # Logging utilities
â”œâ”€â”€ configs/                    # Configuration files
â”œâ”€â”€ docs/                       # Documentation
â”‚   â”œâ”€â”€ DEMO_SUMMARY.md         # Demo overview
â”‚   â”œâ”€â”€ QUICKSTART.md           # Quick start guide
â”‚   â”œâ”€â”€ WANDB_SETUP_COMPLETE.md # WandB setup guide
â”‚   â””â”€â”€ INFERENCE_VALIDATION_FRAMEWORK.md # ğŸ“– New framework docs
â”œâ”€â”€ wandb_integration/          # Weights & Biases integration
â”‚   â”œâ”€â”€ setup_wandb_server.py   # WandB server setup
â”‚   â”œâ”€â”€ train_with_wandb.py     # Training with WandB
â”‚   â””â”€â”€ ...                     # Other WandB utilities
â”œâ”€â”€ notebooks/                  # Jupyter notebooks for analysis
â”œâ”€â”€ results/                    # Unified output directory (gitignored)
â”‚   â”œâ”€â”€ <experiment_name>/      # Individual experiment folders
â”‚   â”‚   â”œâ”€â”€ models/            # Trained models and checkpoints
â”‚   â”‚   â”œâ”€â”€ logs/              # Training and validation logs
â”‚   â”‚   â”œâ”€â”€ visualizations/    # Generated plots and charts
â”‚   â”‚   â””â”€â”€ metrics/           # Performance metrics and summaries
â”‚   â”œâ”€â”€ inference/             # Inference results and monitoring
â”‚   â”œâ”€â”€ validation/            # Model validation reports
â”‚   â””â”€â”€ deployment/            # Deployment artifacts and monitoring
â”œâ”€â”€ inference_validation_demo.py # ğŸ¯ Framework demonstration
â””â”€â”€ test_framework.py           # ğŸ§ª Framework tests
```

## Quick Start

1. Install dependencies:
```bash
pip install -r requirements.txt
```

2. Run CPU allocation training:
```bash
python src/main.py --config_name=cpu_allocation
```

3. View results in TensorBoard:
```bash
tensorboard --logdir results/
```

For more detailed setup instructions, see `docs/QUICKSTART.md`.

## Documentation

Complete documentation is available in the `docs/` folder:
- `docs/QUICKSTART.md` - Detailed setup and usage guide
- `docs/DEMO_SUMMARY.md` - Overview of demo features
- `docs/WANDB_SETUP_COMPLETE.md` - Weights & Biases integration guide

## Weights & Biases Integration

WandB integration code is organized in the `wandb_integration/` folder. This includes:
- Training scripts with WandB logging
- Visualization utilities
- Server setup and configuration
- Demo examples

See `wandb_integration/README.md` for detailed information.

## Usage

### Basic Training Pipeline

```python
from src.core.pipeline import TrainingPipeline
from src.models.cpu_allocation import CPUAllocationModel

# Initialize pipeline
pipeline = TrainingPipeline(config_path="configs/cpu_allocation.yaml")

# Train model
trained_model = pipeline.train()

# Evaluate model
results = pipeline.evaluate(trained_model)
```

### ğŸ”® Advanced Inference and Validation

```python
from src.core.inference import InferenceEngine
from src.core.validation import ModelValidator
from src.core.deployment import ModelDeploymentManager

# Advanced inference with monitoring
inference_engine = InferenceEngine(model, config)
inference_engine.set_reference_statistics(train_loader)

# Run inference with validation
results = inference_engine.run_inference(
    input_data,
    return_confidence=True,
    validate_inputs=True
)

# Comprehensive model validation
validator = ModelValidator(model, config)
validation_results = validator.comprehensive_validation(
    train_loader, val_loader, test_loader
)

# Production deployment
deployment_manager = ModelDeploymentManager(model, config)
deployment_results = deployment_manager.prepare_for_deployment(
    train_loader, val_loader, test_loader
)

# Start serving
deployment_manager.serve_model(enable_monitoring=True)
```

### Custom Model Implementation

```python
from src.core.base_model import BaseModel

class CustomModel(BaseModel):
    def __init__(self, config):
        super().__init__(config)
        # Your model implementation
    
    def forward(self, x):
        # Forward pass implementation
        pass
```

## ğŸš€ New: Inference and Validation Framework

The framework now includes comprehensive inference and validation capabilities for production deployment:

### Quick Test
```bash
# Test the framework
python test_framework.py

# Run full demonstration
python inference_validation_demo.py
```

### Key Features
- **ğŸ”® InferenceEngine**: Advanced inference with monitoring, confidence scoring, and drift detection
- **ğŸ” ModelValidator**: Comprehensive validation pipelines with robustness testing
- **ğŸš€ DeploymentManager**: Production-ready deployment with health monitoring
- **ğŸ“Š Performance Analytics**: Real-time monitoring and automated reporting
- **ğŸ›¡ï¸ Data Validation**: Input quality assessment and validation
- **ğŸ“ˆ Interpretability**: Feature importance and explainability support

### Documentation
- **ğŸ“– Complete Guide**: `docs/INFERENCE_VALIDATION_FRAMEWORK.md`
- **ğŸ¯ Demo Script**: `inference_validation_demo.py`
- **ğŸ§ª Tests**: `test_framework.py`

## Configuration

Models and training parameters are configured via YAML files in the `configs/` directory. See `configs/cpu_allocation.yaml` for an example.

### New Configuration Options
```yaml
# Inference configuration
inference:
  batch_size: 32
  confidence_threshold: 0.8
  drift_threshold: 0.1

# Validation configuration  
validation:
  degradation_threshold: 0.05
  consistency_score: 0.8

# Deployment configuration
deployment:
  enable_monitoring: true
  monitoring_interval: 3600
```

## License

MIT License
